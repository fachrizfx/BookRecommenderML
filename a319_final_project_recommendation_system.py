# -*- coding: utf-8 -*-
"""A319 Final Project - Recommendation System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19kQzexOCO8yKHRcxjQE1BrPN3J-ST6x4

# Data Loading
seperti pada semua kasus/project kita harus mengawali dengan import library yang kita butuhkan pada satu cell ataupun pada masing-masing cell. Pada kasus ini saya akan mengimpornya pada satu cell untuk mempermudah :).
"""

import pandas as pd
import numpy as np 
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt
from google.colab import files
import os
import zipfile
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

"""karena disini saya menggunakan dataset yang berasar dari Kaggle, maka saya akan mendownload dataset melalui public API dari Kaggle. Untuk menggunakannya kita harus memasukan ID kita yang berbentuk file JSON terlebih dahulu seperti berikut."""

files.upload()

!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!ls ~/.kaggle
!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download -d ruchi798/bookcrossing-dataset

local_zip = '/content/bookcrossing-dataset.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content')
zip_ref.close()

books = pd.read_csv('/content/Books Data with Category Language and Summary/Preprocessed_data.csv')
books

books.isnull().sum()

"""Setelah diload kita lihat bahwa dataset kita memiliki size yang cukup besar yaitu 1031175 atau sekitar 1 jutaan. Sebelum kita masuk ke tahap Univariate Analysis, kita akan drop row 'Unamed: 0', 'img_s', 'img_m', 'img_l', 'Summary', 'location' karena column tersebut tidak terlalu berguna. Alasa column 'location' juga di drop adalah dikarenakan column tersebut sudah direpresentasikan oleh column 'city', 'state', 'country'."""

books.drop(['Unnamed: 0', 'img_s', 'img_m', 'img_l', 'Summary', 'location'], inplace=True, axis=1)
books

"""Halo! Saya menulis ini pada tahap menghitung Cosine Similarity. Jadi pada saat saya ingin menghitung cosine similarity dari project ini, saya menemukan masalah, yaitu runtime saya terus-menerus crashing dikarenakan RAM usagenya sudah maksimal, jika saya mengaktifkan Hardware accelerator GPU muncul error bahwa tidak bisa connect dengan GPU. Hal ini bisa saja dikarenakan saya sering menggunakan GPU ataupun ada hal-hal lain. Oleh karena itu saya disini akan mereduksi dataset size dengan cara drop 50%. Saya sudah mencoba mereduksi < 40% tetapi hasilnya tetap sama yaitu runtime crash."""

books.drop(index=range(500000, 1031175), inplace=True, axis=0)
books

"""# Univariate Data Analysis
Sebelum kita masuk kedalam tahap Data Preparation, kita bisa memasuki tahap Data Analysis terlebih dahulu untuk mengetahui data pada dataset ini. Pada tahap ini saya akan menggunakan teknik Univariate Data Analysis. Teknik ini adalah teknik paling dasar untuk menganalysis data. Secara singkat Uni berarti satu, yang berarti menganalysis data secara terpisah (satu per satu). Tujuannya adalah untuk melihat dan memberikan insight mengenai data kita. Karena data kita sudah di rangkum maka kita tidak bisa melakukan analysis secara terpisah.
"""

books.info()

books.hist()

books.isnull().sum()

print('jumlah data: ', len(books))
print('skala rating dari {0} sampai {1}'.format(books['rating'].min(), books['rating'].max()))
print('banyak kategori buku: ', len(books['Category'].unique()))
print('macam-macam bahasa dalam buku: ', books['Language'].unique())

books[books['Language']=='9']

"""Kita dapat simpulkan bahwa column 'city', 'state', dan 'country' terdapat nilai yang bernilai NaN, ini berarti pada column tersebut terdapat missing value. Jika kita perhatikan lagi kita bisa lihat bahwa pada column 'Language', dan 'Category' juga terdapat missing value yang direpresentasikan dengan nilai '9', nilai tersebut dapat dibilang missing value dikarenakan tidak mungkin column 'Language', dan 'Category' memiliki nilai yang sama yaitu '9'. Jumlah dari colum yang memiliki nilai '9' ini juga cukup banyak yaitu sebesar 398937. Kita akan tangani semua missing value ini pada tahap Data Cleaning. Selanjutnya kita akan masuk pada tahap Data Preparation

# Data Preparation
Pada tahap Data Preparation, disini saya tidak akan melakukan teknik-teknik untuk preparasi karena dataset yang saya gunakan ini sudah tergabung semua, atau bisa dibilang siap digunakan dan hanya perlu melakukan preprocessing dan cleaning dari missing value. Oleh karena itu disini saya tidak akan melakukan apa-apa selain dari melakukan teknik Sorting dengan fungsi .sort_value
"""

books = books.sort_values('isbn', ascending=True)
books

"""# Data Cleaning
Seperti yang sudah dibilang pada bagian Univariate Analysis, data kita memiliki missing value yang cukup banyak. Oleh karena itu pada tahap ini kita akan mengani missing value. 

Untuk menangani
"""

books.isnull().sum()

books = books.replace(to_replace='9', value=np.nan)
books[books['Language']=='9'].sum()

len(books[books['Language']=='9'])
books.isnull().sum()

"""Sekarang kita bisa lihat bahwa kita memiliki missing value yang sangat banyak yaitu sebesar 406102! Selanjutnya kita akan drop semua nilai NaN."""

books = books.dropna()

books.isnull().sum()

print('after data cleaning dataset size:', len(books))

len(books['Category'].unique())

"""Sekarang data kita sudah bersih, tetapi kita mengalami data loss lebih dari 40%, yang dapat dibilang cukup signifikan. Selanjutnya kita akan drop semua ISBN yang duplika, hal ini dilakukan agar hanya ada buku yang unique saja. Kita akan drop dengan fungsi .drop_duplicates() [06](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html). """

preparation = books
preparation = preparation.drop_duplicates('isbn')
preparation

"""## Feature Selection"""

references = ['isbn', 'book_title', 'book_author', 'publisher', 'Language', 'Category']

book_isbn = preparation[references[0]].tolist()
 
book_title = preparation[references[1]].tolist()
 
book_author = preparation[references[2]].tolist()

book_publisher = preparation[references[3]].tolist()

book_lang = preparation[references[4]].tolist()

book_category = preparation[references[5]].tolist()
 
print(len(book_isbn))
print(len(book_title))
print(len(book_author))
print(len(book_publisher))
print(len(book_lang))
print(len(book_category))

books_new = pd.DataFrame({
    'isbn': book_isbn,
    'title': book_title,
    'author': book_author,
    'publisher': book_publisher,
    'language': book_lang,
    'category': book_category
})
books_new

"""# Modelling"""

data = books_new

vec = CountVectorizer()
 
vec.fit(data['category']) 
 
vec.get_feature_names()

vec_matrix = vec.fit_transform(data['category']) 
 
vec_matrix.shape

vec_matrix.todense()

"""# Cosine Similarity"""

cosine_sim = cosine_similarity(vec_matrix) 
cosine_sim

cosine_sim_df = pd.DataFrame(cosine_sim, index=data['title'], columns=data['title'])
print('Shape:', cosine_sim_df.shape)
 
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

def get_recommendations(book_title, similarity_data=cosine_sim_df, items=data[['title', 'category']], k=5):
    index = similarity_data.loc[:,book_title].to_numpy().argpartition(
        range(-1, -k, -1))
    
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    
    closest = closest.drop(book_title, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(k)

"""# Evaluation"""

books.sample(10)

books[books['book_title']=='Dragonshadow']

get_recommendations('Dragonshadow', k=10)

def get_precision(relev, sum):
    percentage = print('Recommendation System Precision Percentage: {0}%'.format((relev / sum * 100)))
    return percentage

get_precision(10, 10)

"""Mari kita coba dengan buku yang lain."""

get_recommendations('Under the Tuscan Sun: At Home in Italy', k=20)

get_precision(20, 20)